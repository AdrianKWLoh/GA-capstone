{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales transactions dominate the media in the Singapore property scene, while the rental market does not attract as much attention. I attempt to address this information gap by identifying factors that drive the rental price for private residential property market in Singapore.\n",
    "\n",
    "A good understanding and robust prediction will help house owners to place their asking rent suitably, while potential tenants can benefit by short listing units that suit their budget.\n",
    "\n",
    "This notebook is the first part of the project workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "\n",
    "We begin by scraping the data we need for our analysis from property portals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "# sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Resizing print option to see all columns at once\n",
    "pd.set_option('max_columns', 82)\n",
    "pd.set_option('max_rows', 82)\n",
    "\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import copy\n",
    "import urllib\n",
    "import requests\n",
    "import pickle\n",
    "from time import sleep\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-adbbdccfc998>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# sleep(5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Ensure that the website returned some results before we proceed any further\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;34m\"No rental listings found\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Extract the property results count, discard thousand separators, then cast as integer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We will scrape condominium listings from SRX property portal\n",
    "# We will navigate by district, the portal has a neat & convenient url to do that\n",
    "# 1st placeholder {} for district number, 2nd placeholder {} for page number\n",
    "# url = \"https://www.srx.com.sg/search/rent/condo?selectedDistrictIds={}&page={}\"\n",
    "# Sort postings by new to old\n",
    "url = \"https://www.srx.com.sg/search/rent/condo?selectedDistrictIds={}&orderCriteria=datePostedDesc&page={}\"\n",
    "\n",
    "# Manually set district to scrape, 1 <= district <= 28\n",
    "district = 28\n",
    "page = 1\n",
    "# Search results starts from page 1 for this portal\n",
    "\n",
    "chromedriver = \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(executable_path=chromedriver)\n",
    "\n",
    "# Call webpage\n",
    "driver.get(url.format(district, page))\n",
    "# Webpage needs time to load!\n",
    "sleep(5) # feature rich website, better give more time to load!\n",
    "\n",
    "# Ensure that the website returned some results before we proceed any further\n",
    "assert \"No rental listings found\" not in driver.page_source\n",
    "\n",
    "# Extract the property results count, discard thousand separators, then cast as integer\n",
    "propertycount = Selector(text=driver.page_source).xpath(\"//div[@class='has-properties']/strong[1]/text()\").extract()\n",
    "propertycount = int(propertycount[0].replace(',',''))\n",
    "\n",
    "# Website conveniently provided number of pages for the results, let's grab that too.\n",
    "pages = Selector(text=driver.page_source).xpath(\"//div[@class='has-properties']/strong[3]/text()\").extract()\n",
    "pages = int(pages[0].replace(',',''))\n",
    "\n",
    "print('Found {} condo results in {} pages for district {}.'.format(propertycount, pages, district))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing URLs from page 1 of 5...\n",
      "Grabbing URLs from page 2 of 5...\n",
      "Grabbing URLs from page 3 of 5...\n",
      "Grabbing URLs from page 4 of 5...\n",
      "Grabbing URLs from page 5 of 5...\n",
      "Total URLs grabbed: 89\n",
      "\n",
      "Exporting grabbed URLs to CSV...\n",
      "Export done! Saved as ./dataset/condo_links_D28.csv.\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This cell may take a long time to run!!\n",
    "# We need to go page by page and pull off those 20 links repeatedly\n",
    "baseurl = 'https://www.srx.com.sg'\n",
    "condo_links = []\n",
    "\n",
    "# Results page count starts from 1 on the website URL\n",
    "for page in range(pages):\n",
    "    page += 1  # Need this workaround because page count starts from 1, not 0.\n",
    "    print('Grabbing URLs from page {} of {}...'.format(page, pages))\n",
    "    driver.get(url.format(district, page))\n",
    "    sleep(5)\n",
    "    \n",
    "    soup = driver.page_source\n",
    "    soup = BeautifulSoup(soup, 'lxml')\n",
    "    \n",
    "    # URL is embedded in each picture object, we'll start from there\n",
    "    for item in soup.find_all(\"div\", {\"class\" : \"col-xs-12 col-sm-6 col-md-6 listingPhotoMain\"}):\n",
    "        # Under that picture object, get the first child 'a href' object and read the href link\n",
    "        condo_links.append(baseurl + item.find_next(\"a\").get(\"href\"))\n",
    "    \n",
    "print(\"Total URLs grabbed:\", len(condo_links))\n",
    "\n",
    "# Export and backup the created list as CSV\n",
    "print(\"\\nExporting grabbed URLs to CSV...\")\n",
    "filename = 'condo_links_D{}.csv'.format(district)\n",
    "path = './dataset/'+filename\n",
    "pd.DataFrame(condo_links, columns=['URL']).to_csv(path, index=False)\n",
    "print(\"Export done! Saved as {}.\".format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to store job data\n",
    "condo_df = pd.DataFrame(columns=[\"condo_id\",\"URL\",\"prop_name\",\"prop_type\",\"district\",\"bed\",\"bath\",\"furnish\",\n",
    "                                 \"area\",\"tenure\",\"unit_count\",\"built_year\",\"date_avail\",\"room_type\",\"lease\",\n",
    "                                 \"model\",\"developer\",\"address\",\"latitude\",\"longitude\",\"description\",\"psf\",\"rent\"])\n",
    "\n",
    "# Feature list\n",
    "features = ['Property Name','Property Type','Asking','PSF','Built Year','Date Available From','Room Type',\n",
    "            'Lease Term','Model','Developer','Address','District','Bedrooms','Bathrooms','Furnish','Area',\n",
    "            'Land Tenure','No. of Units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find condo features in posting webpage\n",
    "def grab(feature):\n",
    "    # Some posting may omit certain feature, set this as default sentinel value\n",
    "    result = 'EMPTY'\n",
    "    \n",
    "    # The 18 features we want to grab are conveniently located within this object   \n",
    "    for i in range(len(condo_info.find_all('span', {'class':'listing-about-main-key'}))):\n",
    "        try:\n",
    "            if feature in condo_info.find_all('span', {'class':'listing-about-main-key'})[i].text:\n",
    "                # The key/value pairs are placed side by side, so we have a neat way to access it.\n",
    "                result = condo_info.find_all('span', {'class':'listing-about-main-key'})[i].find_next_sibling().text\n",
    "        except:\n",
    "            result = 'FAIL'  # Set a sentinel value for flagging any weird problem\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing condo data...\n",
      "10 condo records grabbed\n",
      "20 condo records grabbed\n",
      "30 condo records grabbed\n",
      "40 condo records grabbed\n",
      "50 condo records grabbed\n",
      "60 condo records grabbed\n",
      "70 condo records grabbed\n",
      "80 condo records grabbed\n",
      "\n",
      "Total condo records grabbed: 89\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This cell may take a long time to run!!\n",
    "# Keep track of the URLs skipped due to errors/exception\n",
    "skiplist = []\n",
    "\n",
    "print(\"Grabbing condo data...\")\n",
    "for link in condo_links:\n",
    "    # If webpage cannot be loaded for whatever reasons, we will just skip it\n",
    "    try:\n",
    "        driver.get(link)\n",
    "    except:\n",
    "        print(\"Skipping: \", link)\n",
    "        skiplist.append(link)\n",
    "        continue\n",
    "    sleep(5)\n",
    "    soup = driver.page_source\n",
    "    soup = BeautifulSoup(soup, 'lxml')\n",
    "    \n",
    "    # Grab the listing id\n",
    "    try:\n",
    "        condo_id = soup.find('input', {'class':'listingId'}).get('value')\n",
    "    except:\n",
    "        condo_id = 'EMPTY'\n",
    "    \n",
    "    # Grab the GPS coordinates\n",
    "    try:\n",
    "        latitude = soup.find('input', {'id':'sideLatitude'}).get('value')\n",
    "        longitude = soup.find('input', {'id':'sideLongitude'}).get('value')\n",
    "    except:\n",
    "        latitude =  'EMPTY'\n",
    "        longitude = 'EMPTY'\n",
    "    \n",
    "    # The 18 features we want to grab are conveniently located within this object\n",
    "    condo_info = soup.find('div', {'class':'listing-about-main row'})  # grab() reference this object\n",
    "    \n",
    "    prop_name  = grab(features[0])\n",
    "    prop_type  = grab(features[1])\n",
    "    rent      = grab(features[2])\n",
    "    psf        = grab(features[3]).strip()\n",
    "    builtyr    = grab(features[4])\n",
    "    date_avail = grab(features[5])\n",
    "    room_type  = grab(features[6])\n",
    "    lease      = grab(features[7])\n",
    "    model      = grab(features[8])\n",
    "    developer  = grab(features[9])\n",
    "    address    = grab(features[10])\n",
    "    district   = grab(features[11]).strip()\n",
    "    bed        = grab(features[12])\n",
    "    bath       = grab(features[13])\n",
    "    furnish    = grab(features[14])\n",
    "    area       = grab(features[15]).strip()\n",
    "    tenure     = grab(features[16])\n",
    "    unit_count = grab(features[17])\n",
    "    \n",
    "    # Condo description is found in another object, but it contains a child node which we do not need\n",
    "    # Make a copy first since we will take a destructive action, we do not want to alter the original page\n",
    "    try:\n",
    "        desc = copy.copy(soup.find('div', {'id':'listingDesc'}))\n",
    "        desc.find('div', {'class':'listingDetailRoom'}).decompose()  # Prune unwanted child node \n",
    "        description = desc.text.strip()\n",
    "    except:\n",
    "        description = 'EMPTY'\n",
    "    \n",
    "    # Append to the DataFrame.\n",
    "    condo_df.loc[len(condo_df)] = [condo_id, link, prop_name, prop_type, district, bed, bath, furnish, \n",
    "                                   area, tenure, unit_count, builtyr, date_avail, room_type, lease, model, \n",
    "                                   developer, address, latitude, longitude, description, psf, rent]\n",
    "    \n",
    "    if (len(condo_df) % 10) == 0:\n",
    "        print(len(condo_df), \"condo records grabbed\")\n",
    "\n",
    "print(\"\\nTotal condo records grabbed:\", len(condo_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 23)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condo_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting grabbed condo data to CSV...\n",
      "Export done! Saved as ./dataset/condo_data_D28.csv.\n"
     ]
    }
   ],
   "source": [
    "# Export and backup the scraped condo data as CSV\n",
    "print(\"\\nExporting grabbed condo data to CSV...\")\n",
    "filename = 'condo_data_{}.csv'.format(district.split()[0])\n",
    "# filename = 'condo_data_D2.csv'\n",
    "path = './dataset/'+filename\n",
    "condo_df.to_csv(path, index=False)\n",
    "print(\"Export done! Saved as {}.\".format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adrian's Comments:\n",
    "This conclude the section on web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
